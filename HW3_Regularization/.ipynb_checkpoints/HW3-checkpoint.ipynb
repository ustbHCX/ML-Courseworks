{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression \n",
    "在这个实验中，是以Logistic回归作为基础，将再次复习Logistic回归，对Logistic回归将有更深的理解。通过对比未进行正则化的Logistic回归与正则化的Logistic回归在相同数据集上的表现来理解正则化缓解过拟合现象的作用。\n",
    "注：本次实验不再给出理论结果，在你们的训练结果中需要看出加入正则项以后的结果变化。\n",
    "## 1. 导入Python库\n",
    "首先，我们导入这次实验所需要使用的Python库，以及辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 知识回顾--过拟合问题分析\n",
    "\n",
    "实际应用中容易出现过拟合，其原因则在于模型已经足够复杂，但是我们往往根本就不知道设计的模型的复杂程度是否刚好满足要求。\n",
    "\n",
    "这就需要我们去判断模型是否刚刚好，如何判断是否出现了过拟合或欠拟合呢？我们一般通过将数据分为3个部分，训练集(train set)，验证集(validation set)和测试集(test set)。所谓过拟合就是指模型的泛化能力不强，那么，我们就在验证集上测试模型的泛化能力。如下图所示，我们可以看到，过拟合的时候在验证集上表现不好(即泛化能力不强)。而对于欠拟合，往往在训练集上的表现就可以看出表现不好。\n",
    "![goodfit_overfit](images/goodfit_overfit.jpg)\n",
    "\n",
    "如何解决欠拟合和过拟合问题？  \n",
    "欠拟合(Large Bias)： 增加模型的复杂度\n",
    "- 收集新的特征\n",
    "- 增加多项式组合特征   \n",
    "\n",
    "过拟合(Large Variance)\n",
    "- 增加数据(very effective, but not always practical)\n",
    "- 降低模型复杂度：\n",
    "    - 减少特征\n",
    "    - #### 正则化(Regularization)：非常有效的方法，可大幅度降低方差（增加偏差）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 可视化数据\n",
    "\n",
    "为了方便可视化，我们选用二维的数据方便观察。接下来，我们导入这次实验需要用到的数据，并且对其进行可视化。\n",
    "设$X$为我们的特征矩阵，$x^{(i)}$为训练集里面的第$i$个样本，$x_j$为样本中的第$j$个特征，则：  \n",
    "$$X=\\begin{bmatrix}x_1^{(1)} & x_2^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} \\\\ \\vdots & \\vdots \\\\ x_1^{(m)} & x_2^{(m)} \\end{bmatrix}$$  \n",
    "$Y$为一个列向量，$y^{(i)}$代表第$i$个样本对应的标签，则：  \n",
    "$$Y=\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}$$  \n",
    "\n",
    "这里我们已经将数据分成训练集(对应train.txt)和验证集(对应val.txt)。下面直观地观察一下训练集的数据分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('train.txt')\n",
    "val_data = np.loadtxt('val.txt')\n",
    "X_train = train_data[:, 0:2].reshape(-1,2)\n",
    "Y_train = train_data[:, 2]\n",
    "X_val = val_data[:, 0:2].reshape(-1,2)\n",
    "Y_val = val_data[:, 2]\n",
    "\n",
    "print(\"The shape of X_train is:\", X_train.shape)\n",
    "print(\"The shape of Y_train is:\", Y_train.shape)\n",
    "\n",
    "plotData(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic与Regularized Logistic\n",
    "现在的任务是使用Logistic对上面的数据集进行分类。根据2中分析，我们可以知道特征较少往往就不能很好拟合数据，而这里只有两个特征，所以这里我们先使用多项式来组合特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 特征映射\n",
    "\n",
    "上面的数据只有两个特征，$x_1$和$x_2$，我们按照作业1里多项式回归的相同步骤，将$x_1$和$x_2$映射为最高为6次的多项式。即：\n",
    "$$mapFeature(x_1,x_2)=\\begin{bmatrix}1 \\\\ x_1 \\\\ x_2 \\\\ x_1^2 \\\\ x_1x_2 \\\\ x_2^2 \\\\ \\vdots \\\\ x_1x_2^5 \\\\ x_2^6 \\end{bmatrix}$$  \n",
    "这里第一维的*1*同作业1线性回归里一样为了方便处理偏置项，将两个特征映射成了$2+3+ \\dots +7=27$个特征。算上*1*则多项式回归的参数个数为$28$个。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_X_train = mapFeature(X_train[:,0], X_train[:,1], degree=6)\n",
    "print(\"After mapping the features, the shape of map_X_train is:\", map_X_train.shape)\n",
    "\n",
    "print(\"X_train[3]\",X_train[3,:2])\n",
    "print(\"map_X_train[3]\",map_X_train[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 sigmoid函数\n",
    "我们打算使用Logistic回归训练一个模型，来区分我们的正类与负类，因此我们需要一个Sigmoid函数：  \n",
    "$$sigmoid(z) = \\frac{1}{1+e^{-z}}$$\n",
    "**注意**：我们写的Sigmoid函数是需要能够对矩阵直接进行操作的。  \n",
    "**Hint**：计算$e^{-z}$可以使用np.exp(-z)来进行计算  \n",
    "**任务1**：实现sigmoid函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    对矩阵z中每个元素计算其Sigmoid函数值\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    g = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(map_X_train[1, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 初始化参数\n",
    "我们简单地权重$\\theta$初始化为零向量。  \n",
    "$$\\theta = \\begin{bmatrix}\\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} $$  \n",
    "其中$n$为特征的数量。  \n",
    "**Hint**：使用np.zeros()  \n",
    "**任务2**：初始化权重$\\theta$为零向量。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameter(n):\n",
    "    \"\"\"\n",
    "    初始化参数\n",
    "    :param n : map_X_train的列数\n",
    "    :return :权重向量\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    initial_theta = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return initial_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The initialized theta's shape is:\",init_parameter(map_X_train.shape[1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 预测与计算loss\n",
    "\n",
    "没有正则项的loss:\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]}$$ \n",
    "其中，$$h_\\theta(X)=g(X\\theta)\\\\ g(z) = sigmoid(z)$$\n",
    "加入正则项的loss:\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m}{[y^{(i)}log(h_{\\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]}+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\theta_{j}^2}$$  \n",
    "其中，$\\frac{\\lambda}{2m}\\sum_{j=1}^{n}{\\theta_{j}^2}$是正则化项。  \n",
    "我们从上式中看到，将$\\lambda$设置为$0$就可以将有正则项的loss转化为无正则项的loss。因此我们可以来设置$\\lambda$观察有正则和无正则的效果。  \n",
    "\n",
    "预测的时候对于有无正则项都是一样的。\n",
    "$$\n",
    "h_{\\theta}(x^{(i)}) \\ge 0.5 \\Rightarrow 为1类 \\\\\n",
    "h_{\\theta}(x^{(i)}) \\lt 0.5 \\Rightarrow 为0类 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**:  \n",
    "`a = np.array([1,2,3,4])`  \n",
    "`a`的平均值为`a.mean()`或者用`a.sum()`除以`a`的个数。  \n",
    "`a = np.array([0.3,0.5,0.8])` `a.round()`$\\rightarrow$ `[0., 0., 1.]`  \n",
    "其他一些函数可能会有用:`np.dot()`,`np.log()`,`np.power()`  \n",
    "**任务3**：完成计算loss的函数   \n",
    "注意：1.不要`for`循环求和。2.正则项loss不计算第一个权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, theta, lambd):\n",
    "    \"\"\"\n",
    "    计算loss\n",
    "    :param X:特征矩阵X\n",
    "    :param y:特征矩阵X对应的标签\n",
    "    :param theta:权重矩阵theta\n",
    "    :param lambd:正则化参数lambda\n",
    "    :return: loss\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    m = None       #数据的数量\n",
    "    h = None       #h函数\n",
    "    z = None       #正则化项\n",
    "    J = None       #J函数\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([0.1,0.2,0.3,0.4]).reshape(2,2)\n",
    "test_y = np.array([0,1])\n",
    "test_theta = np.array([0.5,0.6])\n",
    "test_lambd = 1\n",
    "print('test loss:',loss(test_X, test_y, test_theta, test_lambd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务4**：预测分类的函数  \n",
    "**Hint**: 可能有用的函数：round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    \"\"\"\n",
    "    对数据矩阵预测类别\n",
    "    :param X:特征矩阵X\n",
    "    :param theta:权重矩阵theta\n",
    "    ：return 由 0.,1.组成的向量，维度应该与X.shape[0]一致\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    h = None\n",
    "    classes = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([-0.1,-0.2,-0.3,0.4]).reshape(2,2)\n",
    "test_theta = np.array([0.5,0.6])\n",
    "print('test predict:',predict(test_X, test_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 计算梯度\n",
    "梯度计算公式如下(可以自己推导一下)：\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_0}= \\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}\\qquad j=0$$  \n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=  \\big[\\frac{1}{m}\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\\big]+\\frac{\\lambda}{m}\\theta_{j} \\quad j\\in\\left\\{ 1,2,...n \\right\\}$$ \n",
    "为了方便，我们可以先对所有$\\theta$(包括$\\theta_0$)用下面的式子求梯度，然后再给$\\theta_0$的梯度减去$\\frac{\\lambda}{m}\\theta_0$  \n",
    "**任务5**：完成计算梯度的函数  \n",
    "**Hint**: 1. 矩阵`A`的转置为`A.T` 2. $\\theta$的维度与$\\frac{\\partial J(\\theta)}{\\partial \\theta}$的维度是一样的 3.矩阵的长宽，或者说向量中的元素个数，可以通过 $X.shape[0]$ 和 $X.shape[1]$ 获得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(X, y, theta, lambd):\n",
    "    \"\"\"\n",
    "    计算梯度\n",
    "    :param X:特征矩阵X\n",
    "    :param y:特征矩阵X对应的标签\n",
    "    :param theta:权重矩阵theta\n",
    "    :param lambd:正则化参数lambda\n",
    "    :return : 对theta的梯度，维度应该与theta一致\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    m = None\n",
    "    h = None\n",
    "    grad = None\n",
    "    grad[0] = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([0.1,0.2,0.3,0.4]).reshape(2,2)\n",
    "test_y = np.array([0,1])\n",
    "test_theta = np.array([0.5,0.6])\n",
    "test_lambd = 1\n",
    "print('test compute_grad:',compute_grad(test_X, test_y, test_theta, test_lambd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 更新参数\n",
    "更新参数还是使用梯度下降法。公式如下：\n",
    "$$\n",
    " \\theta := \\theta - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务6**：完成更新参数的函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pameter(theta, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    更新参数theta\n",
    "    :param theta:权重theta\n",
    "    :param gradients:梯度值\n",
    "    :param learning_rate:学习速率\n",
    "    :return:更新后的theta\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    theta = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([0.1,0.2,0.3,0.4]).reshape(2,2)\n",
    "test_y = np.array([0,1])\n",
    "test_theta = np.array([0.5,0.6])\n",
    "test_lambd = 1\n",
    "test_grad = compute_grad(test_X, test_y, test_theta, test_lambd)\n",
    "print('test update_pameter:',update_pameter(test_theta, test_grad, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 搭积木\n",
    "接下来，我们将上面的代码整合到我们的模型Model中，并且我们将记录下成本$J$的变化过程。  \n",
    "**任务7**：完成训练模型函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(X, y, theta, iteration=300000, learning_rate = 1, lambd = 0):\n",
    "    \"\"\"\n",
    "    Regulared Logistic Regression Model\n",
    "    :param X:输入X\n",
    "    :param y:标签Y\n",
    "    :param theta:参数theta\n",
    "    :param iteration:迭代次数\n",
    "    :param learning_rate:学习率\n",
    "    :param lambd:正则化参数lambda\n",
    "    :return:最终theta的值、theta的历史记录、loss的历史记录和精确度的历史记录\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    J_history = []\n",
    "    acc_history = []\n",
    "    for i in range(iteration):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        gradients = None\n",
    "        theta = None\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        if i%10000==0:\n",
    "            J = loss(X, y, theta, lambd)\n",
    "            J_history.append(J)\n",
    "            pred = predict(X, theta)\n",
    "            acc_history.append((pred==y).mean())\n",
    "            theta_history.append(theta)\n",
    "    \n",
    "    return theta,theta_history, J_history, acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.训练模型与分析\n",
    "\n",
    "## 5.1 无正则项\n",
    "无正则项只需设置$\\lambda=0$即可，下面是无正则项时在训练集和验证集上的表现以及在训练集上的分类边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特征映射\n",
    "map_X_train = mapFeature(X_train[:,0], X_train[:,1], degree=6)\n",
    "map_X_val = mapFeature(X_val[:,0], X_val[:,1], degree=6)\n",
    "# 2. 初始化参数\n",
    "theta = init_parameter(map_X_train.shape[1])\n",
    "# 3. 训练\n",
    "theta,theta_history, J_history, acc_history = Model(map_X_train, Y_train, theta, iteration=300000, learning_rate = 1, lambd = 0)\n",
    "# 4. 验证集上验证\n",
    "acc_val_history = []\n",
    "J_val_history = []\n",
    "for i in range(len(theta_history)):\n",
    "    acc_val = (predict(map_X_val, theta_history[i])==Y_val).mean()\n",
    "    acc_val_history.append(acc_val)\n",
    "    J_val = loss(map_X_val, Y_val, theta_history[i], 0)\n",
    "    J_val_history.append(J_val)\n",
    "# 5. 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 绘制分类边界\n",
    "plotDecisionBoundary(X_train, Y_train,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5.2 比较预测精确度\n",
    "plt.plot(acc_history,label='train')\n",
    "plt.plot(acc_val_history,label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 比较loss\n",
    "plt.plot(J_history,label='train')\n",
    "plt.plot(J_val_history,label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练过程中的分类精确度分析可知，随着训练次数增加，在训练集上的精确度会进一步提升，但是在验证集上的精确度有轻微的下降。  \n",
    "从训练过程中的loss分析可知，随着训练次数增加，在训练集上的loss会进一步降低，但是在验证集上的loss会有些发散。  \n",
    "这些都说明了训练的模型已经过拟合，需要降低模型复杂度来提高泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 有正则项\n",
    "这里设置$\\lambda=0.005$，可以再提交作业后尝试设置不同的值观察结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特征映射\n",
    "map_X_train = mapFeature(X_train[:,0], X_train[:,1], degree=6)\n",
    "map_X_val = mapFeature(X_val[:,0], X_val[:,1], degree=6)\n",
    "# 2. 初始化参数\n",
    "theta = init_parameter(map_X_train.shape[1])\n",
    "# 3. 训练\n",
    "theta,theta_history, J_history, acc_history = Model(map_X_train, Y_train, theta, iteration=300000, learning_rate = 1, lambd = 0.005)\n",
    "# 4. 验证集上验证\n",
    "acc_val_history = []\n",
    "J_val_history = []\n",
    "for i in range(len(theta_history)):\n",
    "    acc_val = (predict(map_X_val, theta_history[i])==Y_val).mean()\n",
    "    acc_val_history.append(acc_val)\n",
    "    J_val = loss(map_X_val, Y_val, theta_history[i], 0)\n",
    "    J_val_history.append(J_val)\n",
    "# 5. 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 绘制分类边界\n",
    "plotDecisionBoundary(X_train, Y_train,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 比较预测精确度\n",
    "plt.plot(acc_history,label='train')\n",
    "plt.plot(acc_val_history,label='validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 5.3 比较loss\n",
    "plt.plot(J_history,label='train')\n",
    "plt.plot(J_val_history,label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比无正则项的实验结果，我们可以发现有正则项的模型明显提升了泛化能力，过拟合的现象大大减小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 总结\n",
    "通过这次实验，我们能够直观的理解正则化对于缓解过拟合现象所起到的作用。在提交完作业后，你还可以试试不同的$\\lambda$值，观察决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ea307f64f28ee5e082e49b3665309add68f7312938244f4050a2f009b4177f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
